{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from langchain_chroma import Chroma \n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "from retriever import Retriever\n",
    "from evaluator import RetrievalEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('sample_score.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['gender', 'race/ethnicity', 'parental level of education', 'lunch',\n",
       "       'test preparation course', 'math score', 'reading score',\n",
       "       'writing score'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 8)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluate retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Embedding dimension 768 does not match collection dimensionality 384\n",
      "Error: Embedding dimension 768 does not match collection dimensionality 384\n"
     ]
    }
   ],
   "source": [
    "modes = ['embed', 'bm25', 'hybrid']\n",
    "embed_model_names = [\"all-MiniLM-L6-v2\", \"BAAI/LLM-Embedder\", \"BAAI/bge-small-en-v1.5\"]\n",
    "dbs = [FAISS, Chroma]\n",
    "db_names = ['faiss', 'chroma']\n",
    "\n",
    "queries = [\"What are the lowest writing scores?\", \n",
    "           \"What are the highest reading scores?\", \n",
    "           \"What are the average math scores?\", \n",
    "           \"If parental level of education has the impact for reading score?\", \n",
    "           \"What's the best comprehensive score?\",\n",
    "           \"If food impacts writing score?\"]\n",
    "\n",
    "relevant_docs = [['writing score'], \n",
    "                 ['reading score'], \n",
    "                 ['math score'], \n",
    "                 ['parental level of education','reading score'], \n",
    "                 ['writing score','reading score', 'math score'],\n",
    "                 ['lunch','writing score']]\n",
    "\n",
    "retriever_results = {'modes': [], \n",
    "                     'embed_model_names': [], \n",
    "                     'dbs': [], \n",
    "                     'Recall@k': [],\n",
    "                     'MRR': [],\n",
    "                     'queries': [],\n",
    "                     'retrieved_docs': []}\n",
    "\n",
    "for mode in modes:    \n",
    "    for embed_model_name in embed_model_names:       \n",
    "        for i in range(len(dbs)):           \n",
    "            try:\n",
    "                retriever = Retriever(mode=mode, embed_model_name=embed_model_name, db=dbs[i], top_k=5)\n",
    "                for j in range(len(queries)):\n",
    "                    retrieved_doc = retriever.retrieve_schema(queries[j], df, evaluate=True)\n",
    "                    relevant_doc = relevant_docs[j]\n",
    "                    evaluator = RetrievalEvaluator(retrieved_doc, relevant_doc)\n",
    "                    results = evaluator.evaluate()\n",
    "                    for key, value in results.items():\n",
    "                        retriever_results[key].append(value)\n",
    "                        \n",
    "                    retriever_results['modes'].append(mode)\n",
    "                    retriever_results['embed_model_names'].append(embed_model_name)\n",
    "                    retriever_results['dbs'].append(db_names[i])\n",
    "                    retriever_results['queries'].append(queries[j])\n",
    "                    retriever_results['retrieved_docs'].append(retrieved_doc)\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "retr_df =pd.DataFrame(retriever_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>recall_5_mean</th>\n",
       "      <th>recall_5_count</th>\n",
       "      <th>mrr_mean</th>\n",
       "      <th>mrr_count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>modes</th>\n",
       "      <th>embed_model_names</th>\n",
       "      <th>dbs</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">bm25</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">BAAI/LLM-Embedder</th>\n",
       "      <th>chroma</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>faiss</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">BAAI/bge-small-en-v1.5</th>\n",
       "      <th>chroma</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>faiss</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">all-MiniLM-L6-v2</th>\n",
       "      <th>chroma</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>faiss</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">embed</th>\n",
       "      <th>BAAI/LLM-Embedder</th>\n",
       "      <th>faiss</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">BAAI/bge-small-en-v1.5</th>\n",
       "      <th>chroma</th>\n",
       "      <td>0.805556</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>faiss</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">all-MiniLM-L6-v2</th>\n",
       "      <th>chroma</th>\n",
       "      <td>0.805556</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>faiss</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">hybrid</th>\n",
       "      <th>BAAI/LLM-Embedder</th>\n",
       "      <th>faiss</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">BAAI/bge-small-en-v1.5</th>\n",
       "      <th>chroma</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>faiss</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">all-MiniLM-L6-v2</th>\n",
       "      <th>chroma</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>faiss</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      recall_5_mean  recall_5_count  mrr_mean  \\\n",
       "modes  embed_model_names      dbs                                               \n",
       "bm25   BAAI/LLM-Embedder      chroma       1.000000               6       1.0   \n",
       "                              faiss        1.000000               6       1.0   \n",
       "       BAAI/bge-small-en-v1.5 chroma       1.000000               6       1.0   \n",
       "                              faiss        1.000000               6       1.0   \n",
       "       all-MiniLM-L6-v2       chroma       1.000000               6       1.0   \n",
       "                              faiss        1.000000               6       1.0   \n",
       "embed  BAAI/LLM-Embedder      faiss        1.000000               6       1.0   \n",
       "       BAAI/bge-small-en-v1.5 chroma       0.805556               6       1.0   \n",
       "                              faiss        1.000000               6       1.0   \n",
       "       all-MiniLM-L6-v2       chroma       0.805556               6       1.0   \n",
       "                              faiss        1.000000               6       1.0   \n",
       "hybrid BAAI/LLM-Embedder      faiss        1.000000               6       1.0   \n",
       "       BAAI/bge-small-en-v1.5 chroma       1.000000               6       1.0   \n",
       "                              faiss        1.000000               6       1.0   \n",
       "       all-MiniLM-L6-v2       chroma       1.000000               6       1.0   \n",
       "                              faiss        1.000000               6       1.0   \n",
       "\n",
       "                                      mrr_count  \n",
       "modes  embed_model_names      dbs                \n",
       "bm25   BAAI/LLM-Embedder      chroma          6  \n",
       "                              faiss           6  \n",
       "       BAAI/bge-small-en-v1.5 chroma          6  \n",
       "                              faiss           6  \n",
       "       all-MiniLM-L6-v2       chroma          6  \n",
       "                              faiss           6  \n",
       "embed  BAAI/LLM-Embedder      faiss           6  \n",
       "       BAAI/bge-small-en-v1.5 chroma          6  \n",
       "                              faiss           6  \n",
       "       all-MiniLM-L6-v2       chroma          6  \n",
       "                              faiss           6  \n",
       "hybrid BAAI/LLM-Embedder      faiss           6  \n",
       "       BAAI/bge-small-en-v1.5 chroma          6  \n",
       "                              faiss           6  \n",
       "       all-MiniLM-L6-v2       chroma          6  \n",
       "                              faiss           6  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retr_df.groupby(['modes', 'embed_model_names', 'dbs'])[['Recall@k', 'MRR']].agg(\n",
    "    recall_5_mean=('Recall@k', 'mean'), \n",
    "    recall_5_count=('Recall@k', 'count'),\n",
    "    mrr_mean=('MRR', 'mean'), \n",
    "    mrr_count=('MRR', 'count')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"embed + Chroma\" isn't a good combination choice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>modes</th>\n",
       "      <th>embed_model_names</th>\n",
       "      <th>dbs</th>\n",
       "      <th>Recall@k</th>\n",
       "      <th>MRR</th>\n",
       "      <th>queries</th>\n",
       "      <th>retrieved_docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>embed</td>\n",
       "      <td>all-MiniLM-L6-v2</td>\n",
       "      <td>chroma</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>What's the best comprehensive score?</td>\n",
       "      <td>[reading score, reading score, reading score, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>embed</td>\n",
       "      <td>all-MiniLM-L6-v2</td>\n",
       "      <td>chroma</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>If food impacts writing score?</td>\n",
       "      <td>[writing score, writing score, writing score, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>embed</td>\n",
       "      <td>BAAI/bge-small-en-v1.5</td>\n",
       "      <td>chroma</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>What's the best comprehensive score?</td>\n",
       "      <td>[writing score, writing score, writing score, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>embed</td>\n",
       "      <td>BAAI/bge-small-en-v1.5</td>\n",
       "      <td>chroma</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>If food impacts writing score?</td>\n",
       "      <td>[writing score, writing score, writing score, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    modes       embed_model_names     dbs  Recall@k  MRR  \\\n",
       "10  embed        all-MiniLM-L6-v2  chroma  0.333333  1.0   \n",
       "11  embed        all-MiniLM-L6-v2  chroma  0.500000  1.0   \n",
       "28  embed  BAAI/bge-small-en-v1.5  chroma  0.333333  1.0   \n",
       "29  embed  BAAI/bge-small-en-v1.5  chroma  0.500000  1.0   \n",
       "\n",
       "                                 queries  \\\n",
       "10  What's the best comprehensive score?   \n",
       "11        If food impacts writing score?   \n",
       "28  What's the best comprehensive score?   \n",
       "29        If food impacts writing score?   \n",
       "\n",
       "                                       retrieved_docs  \n",
       "10  [reading score, reading score, reading score, ...  \n",
       "11  [writing score, writing score, writing score, ...  \n",
       "28  [writing score, writing score, writing score, ...  \n",
       "29  [writing score, writing score, writing score, ...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "retr_df[retr_df[['Recall@k', 'MRR']].sum(axis=1) < 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['reading score',\n",
       "  'reading score',\n",
       "  'reading score',\n",
       "  'reading score',\n",
       "  'reading score'],\n",
       " ['writing score',\n",
       "  'writing score',\n",
       "  'writing score',\n",
       "  'writing score',\n",
       "  'writing score'],\n",
       " ['writing score',\n",
       "  'writing score',\n",
       "  'writing score',\n",
       "  'writing score',\n",
       "  'writing score'],\n",
       " ['writing score',\n",
       "  'writing score',\n",
       "  'writing score',\n",
       "  'writing score',\n",
       "  'writing score']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retr_df[retr_df[['Recall@k', 'MRR']].sum(axis=1) < 2]['retrieved_docs'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the performance is good, just choose one and explain why.\n",
    "\n",
    "Mode: hybrid (to balance semantic richness and exact matching).\n",
    "\n",
    "Embedding Model: BAAI/bge-small-en-v1.5 (for a balance between speed and accuracy).\n",
    "\n",
    "Database: FAISS (for its performance and scalability)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluate code generator\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "from model import Model\n",
    "from retriever import Retriever\n",
    "from prompts import get_prompt, combined_template\n",
    "from agent import CodeRAGAgent\n",
    "from evaluator import GenerCodeEvaluator\n",
    "from execute import extract_code\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('sample_score.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "evaluate code generator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = Retriever(mode='hybrid', embed_model_name=\"BAAI/bge-small-en-v1.5\", db=FAISS, top_k=5)\n",
    "prompt = get_prompt(combined_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [\"llama3.1\", \"llama-3.3-70b-versatile\",\"mistral\", \"gemma2-9b-it\"]\n",
    "\n",
    "Kwargs = [{'stable': {'temperature': 0.1, 'top_p': 0.1}},\n",
    "          {'diverse': {'temperature': 0.9, 'top_p': 0.9}},\n",
    "          {'default': {}}]\n",
    "\n",
    "queries = [\"What are the two lowest writing scores?\", \n",
    "           \"What are the highest reading scores?\", \n",
    "           \"What are the average math scores?\", \n",
    "           \"If parental level of education has the impact for reading score?\", \n",
    "           \"What's the best comprehensive score?\",\n",
    "           \"If food impacts writing score?\"]\n",
    "\n",
    "reference_codes = [\"df['writing score'].nsmallest(2)\", \n",
    "                 \"df['reading score'].max()\", \n",
    "                 \"df['math score'].mean()\", \n",
    "                 \"df.groupby('parental level of education')['reading score'].mean()\", \n",
    "                 \"df[['reading score', 'writing score', 'math score']].sum(axis=1).max()\",\n",
    "                 \"df.groupby('lunch')['writing score'].mean()\"]\n",
    "\n",
    "code_generator_results = {'model_names': [], \n",
    "                         'Exact Match': [],\n",
    "                         'F1 Score': [],\n",
    "                         'queries': [],\n",
    "                         'generateds': [],\n",
    "                         'param_types': []}\n",
    "\n",
    "\n",
    "for model_name in model_names:\n",
    "    for dic_ in Kwargs:\n",
    "        for param_type, kwargs in dic_.items():\n",
    "            model = Model(model_name=model_name, **kwargs)\n",
    "            processor = CodeRAGAgent(retriever, prompt, model, df)\n",
    "            for i in range(len(queries)):\n",
    "                try:\n",
    "                    generated = processor.processor(queries[i])\n",
    "                    generated_code = extract_code(generated)\n",
    "                    evaluator = GenerCodeEvaluator(generated_code, reference_codes[i])\n",
    "                    results = evaluator.evaluate()\n",
    "                    for key, value in results.items():\n",
    "                        code_generator_results[key].append(value)\n",
    "                    \n",
    "                    code_generator_results['model_names'].append(model_name)\n",
    "                    code_generator_results['queries'].append(queries[i])\n",
    "                    code_generator_results['generateds'].append(generated)\n",
    "                    code_generator_results['param_types'].append(param_type)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_df = pd.DataFrame(code_generator_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>em_mean</th>\n",
       "      <th>em_count</th>\n",
       "      <th>f1_mean</th>\n",
       "      <th>f1_count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_names</th>\n",
       "      <th>param_types</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">gemma2-9b-it</th>\n",
       "      <th>default</th>\n",
       "      <td>0.833333</td>\n",
       "      <td>6</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>diverse</th>\n",
       "      <td>0.833333</td>\n",
       "      <td>6</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stable</th>\n",
       "      <td>0.833333</td>\n",
       "      <td>6</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">llama-3.3-70b-versatile</th>\n",
       "      <th>default</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>diverse</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stable</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">llama3.1</th>\n",
       "      <th>default</th>\n",
       "      <td>0.833333</td>\n",
       "      <td>6</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>diverse</th>\n",
       "      <td>0.833333</td>\n",
       "      <td>6</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stable</th>\n",
       "      <td>0.833333</td>\n",
       "      <td>6</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">mistral</th>\n",
       "      <th>default</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>6</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>diverse</th>\n",
       "      <td>0.833333</td>\n",
       "      <td>6</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stable</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>6</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      em_mean  em_count   f1_mean  f1_count\n",
       "model_names             param_types                                        \n",
       "gemma2-9b-it            default      0.833333         6  0.916667         6\n",
       "                        diverse      0.833333         6  0.916667         6\n",
       "                        stable       0.833333         6  0.916667         6\n",
       "llama-3.3-70b-versatile default      1.000000         6  1.000000         6\n",
       "                        diverse      1.000000         6  1.000000         6\n",
       "                        stable       1.000000         6  1.000000         6\n",
       "llama3.1                default      0.833333         6  0.916667         6\n",
       "                        diverse      0.833333         6  0.916667         6\n",
       "                        stable       0.833333         6  0.916667         6\n",
       "mistral                 default      0.666667         6  0.666667         6\n",
       "                        diverse      0.833333         6  0.916667         6\n",
       "                        stable       0.666667         6  0.750000         6"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "gen_df.groupby(['model_names', 'param_types'])[['Exact Match', 'F1 Score']].agg(\n",
    "    em_mean=('Exact Match', 'mean'), \n",
    "    em_count=('Exact Match', 'count'),\n",
    "    f1_mean=('F1 Score', 'mean'), \n",
    "    f1_count=('F1 Score', 'count')\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "llama-3.3-70b-versatile is the best model. diverse is good.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluate interpretation generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "from model import Model\n",
    "from retriever import Retriever\n",
    "from prompts import get_prompt, combined_template, interp_template\n",
    "from agent import CodeRAGAgent, InterpRAGAgent\n",
    "from evaluator import bert_score_f1\n",
    "from execute import parse_response\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('sample_score.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "evaluate interpretation generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = Retriever(mode='hybrid', embed_model_name=\"BAAI/bge-small-en-v1.5\", db=FAISS, top_k=5)\n",
    "prompt = get_prompt(combined_template)\n",
    "code_model = Model(model_name=\"gemma2-9b-it\",temperature=0.9,top_p=0.9)\n",
    "processor = CodeRAGAgent(retriever, prompt, code_model, df)\n",
    "interp_prompt = get_prompt(interp_template)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01jhnywfrpfrhb5r935e6ewy2h` service tier `on_demand` on : Limit 100000, Used 99717, Requested 1721. Please try again in 20m42.364s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': '', 'code': 'rate_limit_exceeded'}}\n",
      "Error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01jhnywfrpfrhb5r935e6ewy2h` service tier `on_demand` on : Limit 100000, Used 99717, Requested 1734. Please try again in 20m53.328s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': '', 'code': 'rate_limit_exceeded'}}\n",
      "Error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01jhnywfrpfrhb5r935e6ewy2h` service tier `on_demand` on : Limit 100000, Used 99717, Requested 1724. Please try again in 20m44.418999999s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': '', 'code': 'rate_limit_exceeded'}}\n",
      "Error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01jhnywfrpfrhb5r935e6ewy2h` service tier `on_demand` on : Limit 100000, Used 99716, Requested 1721. Please try again in 20m41.538s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': '', 'code': 'rate_limit_exceeded'}}\n",
      "Error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01jhnywfrpfrhb5r935e6ewy2h` service tier `on_demand` on : Limit 100000, Used 99716, Requested 1718. Please try again in 20m38.669999999s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': '', 'code': 'rate_limit_exceeded'}}\n",
      "Error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01jhnywfrpfrhb5r935e6ewy2h` service tier `on_demand` on : Limit 100000, Used 99716, Requested 1737. Please try again in 20m54.827s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': '', 'code': 'rate_limit_exceeded'}}\n",
      "Error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01jhnywfrpfrhb5r935e6ewy2h` service tier `on_demand` on : Limit 100000, Used 99716, Requested 1721. Please try again in 20m40.745999999s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': '', 'code': 'rate_limit_exceeded'}}\n",
      "Error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01jhnywfrpfrhb5r935e6ewy2h` service tier `on_demand` on : Limit 100000, Used 99715, Requested 1734. Please try again in 20m51.705s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': '', 'code': 'rate_limit_exceeded'}}\n",
      "Error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01jhnywfrpfrhb5r935e6ewy2h` service tier `on_demand` on : Limit 100000, Used 99715, Requested 1724. Please try again in 20m42.796s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': '', 'code': 'rate_limit_exceeded'}}\n",
      "Error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01jhnywfrpfrhb5r935e6ewy2h` service tier `on_demand` on : Limit 100000, Used 99715, Requested 1721. Please try again in 20m39.941s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': '', 'code': 'rate_limit_exceeded'}}\n",
      "Error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01jhnywfrpfrhb5r935e6ewy2h` service tier `on_demand` on : Limit 100000, Used 99714, Requested 1718. Please try again in 20m37.089s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': '', 'code': 'rate_limit_exceeded'}}\n",
      "Error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01jhnywfrpfrhb5r935e6ewy2h` service tier `on_demand` on : Limit 100000, Used 99714, Requested 1737. Please try again in 20m53.244s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': '', 'code': 'rate_limit_exceeded'}}\n",
      "Error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01jhnywfrpfrhb5r935e6ewy2h` service tier `on_demand` on : Limit 100000, Used 99714, Requested 1721. Please try again in 20m39.157s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': '', 'code': 'rate_limit_exceeded'}}\n",
      "Error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01jhnywfrpfrhb5r935e6ewy2h` service tier `on_demand` on : Limit 100000, Used 99713, Requested 1734. Please try again in 20m50.116s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': '', 'code': 'rate_limit_exceeded'}}\n",
      "Error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01jhnywfrpfrhb5r935e6ewy2h` service tier `on_demand` on : Limit 100000, Used 99713, Requested 1724. Please try again in 20m41.202s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': '', 'code': 'rate_limit_exceeded'}}\n",
      "Error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01jhnywfrpfrhb5r935e6ewy2h` service tier `on_demand` on : Limit 100000, Used 99713, Requested 1721. Please try again in 20m38.353s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': '', 'code': 'rate_limit_exceeded'}}\n",
      "Error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01jhnywfrpfrhb5r935e6ewy2h` service tier `on_demand` on : Limit 100000, Used 99712, Requested 1718. Please try again in 20m35.483s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': '', 'code': 'rate_limit_exceeded'}}\n",
      "Error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01jhnywfrpfrhb5r935e6ewy2h` service tier `on_demand` on : Limit 100000, Used 99712, Requested 1737. Please try again in 20m51.616s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': '', 'code': 'rate_limit_exceeded'}}\n",
      "Error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01jhnywfrpfrhb5r935e6ewy2h` service tier `on_demand` on : Limit 100000, Used 99712, Requested 1721. Please try again in 20m37.511s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': '', 'code': 'rate_limit_exceeded'}}\n",
      "Error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01jhnywfrpfrhb5r935e6ewy2h` service tier `on_demand` on : Limit 100000, Used 99712, Requested 1734. Please try again in 20m48.516s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': '', 'code': 'rate_limit_exceeded'}}\n",
      "Error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01jhnywfrpfrhb5r935e6ewy2h` service tier `on_demand` on : Limit 100000, Used 99711, Requested 1724. Please try again in 20m39.628s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': '', 'code': 'rate_limit_exceeded'}}\n",
      "Error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01jhnywfrpfrhb5r935e6ewy2h` service tier `on_demand` on : Limit 100000, Used 99711, Requested 1721. Please try again in 20m36.747s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': '', 'code': 'rate_limit_exceeded'}}\n",
      "Error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01jhnywfrpfrhb5r935e6ewy2h` service tier `on_demand` on : Limit 100000, Used 99711, Requested 1718. Please try again in 20m33.912s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': '', 'code': 'rate_limit_exceeded'}}\n",
      "Error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01jhnywfrpfrhb5r935e6ewy2h` service tier `on_demand` on : Limit 100000, Used 99710, Requested 1737. Please try again in 20m50.072s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': '', 'code': 'rate_limit_exceeded'}}\n",
      "Error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01jhnywfrpfrhb5r935e6ewy2h` service tier `on_demand` on : Limit 100000, Used 99710, Requested 1721. Please try again in 20m35.993s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': '', 'code': 'rate_limit_exceeded'}}\n",
      "Error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01jhnywfrpfrhb5r935e6ewy2h` service tier `on_demand` on : Limit 100000, Used 99710, Requested 1734. Please try again in 20m46.956s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': '', 'code': 'rate_limit_exceeded'}}\n",
      "Error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01jhnywfrpfrhb5r935e6ewy2h` service tier `on_demand` on : Limit 100000, Used 99709, Requested 1724. Please try again in 20m38.054s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': '', 'code': 'rate_limit_exceeded'}}\n",
      "Error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01jhnywfrpfrhb5r935e6ewy2h` service tier `on_demand` on : Limit 100000, Used 99709, Requested 1721. Please try again in 20m35.217s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': '', 'code': 'rate_limit_exceeded'}}\n",
      "Error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01jhnywfrpfrhb5r935e6ewy2h` service tier `on_demand` on : Limit 100000, Used 99709, Requested 1718. Please try again in 20m32.385s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': '', 'code': 'rate_limit_exceeded'}}\n",
      "Error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01jhnywfrpfrhb5r935e6ewy2h` service tier `on_demand` on : Limit 100000, Used 99709, Requested 1737. Please try again in 20m48.508s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': '', 'code': 'rate_limit_exceeded'}}\n",
      "Error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01jhnywfrpfrhb5r935e6ewy2h` service tier `on_demand` on : Limit 100000, Used 99708, Requested 1721. Please try again in 20m34.387s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': '', 'code': 'rate_limit_exceeded'}}\n",
      "Error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01jhnywfrpfrhb5r935e6ewy2h` service tier `on_demand` on : Limit 100000, Used 99708, Requested 1734. Please try again in 20m45.351999999s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': '', 'code': 'rate_limit_exceeded'}}\n",
      "Error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01jhnywfrpfrhb5r935e6ewy2h` service tier `on_demand` on : Limit 100000, Used 99708, Requested 1724. Please try again in 20m36.436s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': '', 'code': 'rate_limit_exceeded'}}\n",
      "Error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01jhnywfrpfrhb5r935e6ewy2h` service tier `on_demand` on : Limit 100000, Used 99707, Requested 1721. Please try again in 20m33.357s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': '', 'code': 'rate_limit_exceeded'}}\n",
      "Error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01jhnywfrpfrhb5r935e6ewy2h` service tier `on_demand` on : Limit 100000, Used 99706, Requested 1718. Please try again in 20m30.198s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': '', 'code': 'rate_limit_exceeded'}}\n",
      "Error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01jhnywfrpfrhb5r935e6ewy2h` service tier `on_demand` on : Limit 100000, Used 99706, Requested 1737. Please try again in 20m46.335s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': '', 'code': 'rate_limit_exceeded'}}\n",
      "Error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01jhnywfrpfrhb5r935e6ewy2h` service tier `on_demand` on : Limit 100000, Used 99706, Requested 1721. Please try again in 20m32.251s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': '', 'code': 'rate_limit_exceeded'}}\n",
      "Error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01jhnywfrpfrhb5r935e6ewy2h` service tier `on_demand` on : Limit 100000, Used 99705, Requested 1734. Please try again in 20m43.23s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': '', 'code': 'rate_limit_exceeded'}}\n",
      "Error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01jhnywfrpfrhb5r935e6ewy2h` service tier `on_demand` on : Limit 100000, Used 99705, Requested 1724. Please try again in 20m34.35s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': '', 'code': 'rate_limit_exceeded'}}\n",
      "Error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01jhnywfrpfrhb5r935e6ewy2h` service tier `on_demand` on : Limit 100000, Used 99705, Requested 1721. Please try again in 20m31.494s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': '', 'code': 'rate_limit_exceeded'}}\n",
      "Error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01jhnywfrpfrhb5r935e6ewy2h` service tier `on_demand` on : Limit 100000, Used 99705, Requested 1718. Please try again in 20m28.640999999s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': '', 'code': 'rate_limit_exceeded'}}\n"
     ]
    }
   ],
   "source": [
    "model_names = [\"llama-3.3-70b-versatile\",\"llama3.1\", \"mistral\", \"gemma2-9b-it\"]\n",
    "\n",
    "Kwargs = [{'stable': {'temperature': 0.1,'top_p': 0.1}},\n",
    "          {'diverse': {'temperature': 0.9, 'top_p': 0.9}},\n",
    "          {'default': {}}]\n",
    "\n",
    "queries = [\"What are the two lowest writing scores?\", \n",
    "           \"What are the highest reading scores?\", \n",
    "           \"What are the average math scores?\", \n",
    "           \"If parental level of education has the impact for reading score?\", \n",
    "           \"What's the best comprehensive score?\",\n",
    "           \"If food impacts writing score?\"]\n",
    "\n",
    "references = [[\"The two lowest writing scores are 22 and 41, associated with indices 11 and 9, respectively, indicating that these are the lowest writing scores in the dataset.\"], \n",
    "              [\"The highest reading scores achieved are 86.\"], \n",
    "              [\"The calculated result indicates that the average math scores among the given data is approximately 63.85.\"], \n",
    "              [\"The relative result indicates that there is a significant difference in reading scores across different levels of parental education, suggesting that parental level of education does have an impact on reading performance.\"], \n",
    "              [\"The best comprehensive score is 261, indicating that the sum of reading, writing, and math scores for this particular group or individual is the highest among all available data.\"],\n",
    "              [\"The relative data suggests that students who received free or reduced lunch had a significantly higher mean writing score (65.7) compared to those who received standard lunch (62.2), indicating a positive impact of food on writing performance.\"]]\n",
    "\n",
    "generator_results = {'model_names': [], \n",
    "                    'bert_score_f1': [],\n",
    "                    'queries': [],\n",
    "                    'responses': [],\n",
    "                    'param_types': []}\n",
    "\n",
    "for model_name in model_names:\n",
    "    for dic_ in Kwargs:\n",
    "        for param_type, kwargs in dic_.items():\n",
    "            model = Model(model_name=model_name, **kwargs)\n",
    "            interp = InterpRAGAgent(interp_prompt, model)\n",
    "            for i in range(len(queries)):\n",
    "                try:\n",
    "                    ctx = processor.invoke(queries[i])\n",
    "                    answer = interp.processor(ctx, queries[i])\n",
    "                    response = parse_response(answer)['The concluding response:']\n",
    "                    f1 = bert_score_f1([response], references[i])\n",
    "                    generator_results['bert_score_f1'].append(f1)\n",
    "                    generator_results['model_names'].append(model_name)\n",
    "                    generator_results['queries'].append(queries[i])\n",
    "                    generator_results['responses'].append(response)\n",
    "                    generator_results['param_types'].append(param_type)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp_df = pd.DataFrame(generator_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>f1_mean</th>\n",
       "      <th>f1_count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_names</th>\n",
       "      <th>param_types</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">gemma2-9b-it</th>\n",
       "      <th>default</th>\n",
       "      <td>0.826000</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>diverse</th>\n",
       "      <td>0.813333</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stable</th>\n",
       "      <td>0.813333</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">llama3.1</th>\n",
       "      <th>default</th>\n",
       "      <td>0.860000</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>diverse</th>\n",
       "      <td>0.856667</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stable</th>\n",
       "      <td>0.890000</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">mistral</th>\n",
       "      <th>default</th>\n",
       "      <td>0.812000</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>diverse</th>\n",
       "      <td>0.790000</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stable</th>\n",
       "      <td>0.822000</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           f1_mean  f1_count\n",
       "model_names  param_types                    \n",
       "gemma2-9b-it default      0.826000         5\n",
       "             diverse      0.813333         6\n",
       "             stable       0.813333         6\n",
       "llama3.1     default      0.860000         6\n",
       "             diverse      0.856667         6\n",
       "             stable       0.890000         6\n",
       "mistral      default      0.812000         5\n",
       "             diverse      0.790000         5\n",
       "             stable       0.822000         5"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interp_df.groupby(['model_names', 'param_types'])[['bert_score_f1']].agg(\n",
    "    f1_mean=('bert_score_f1', 'mean'), \n",
    "    f1_count=('bert_score_f1', 'count')\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "llama-3.3-70b-versatile is the best model. stable is fine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
